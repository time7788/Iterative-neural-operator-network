{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500181b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d459b71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_NUM = 4\n",
    "cross_section_n = 1\n",
    "train_num = 3\n",
    "test_num = 1\n",
    "size1 = 128\n",
    "size2 = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e0860c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Iteration_NUM = 29 # 迭代步数\n",
    "DE_NUM = 12 #隐藏层特征图的个数\n",
    "DE_kernel_size = 37 #卷积核大小\n",
    "out_channel = 1 #输出\n",
    "hs_channel = 7 #微分方程的个数-1\n",
    "bc_channel = 4 #边界条件的图层数\n",
    "INPUT1_SHAPE = [None,None, bc_channel]\n",
    "OUTPUT_SHAPE = [None,None, out_channel]\n",
    "HS_SHAPE = [None,None, hs_channel] \n",
    "MIX_SHAPE = [None,None, hs_channel+out_channel]\n",
    "\n",
    "LAMBDA = 100 #L1损失的权重\n",
    "# WEIGHT_SSIM = 10 #图像结构相似度损失的权重\n",
    "G_step_size = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4272bb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data path\n",
    "input_path = \"./dataset/\" # 数据输入路径\n",
    "save_path = \"./output3\" #输出结果图片保存路径\n",
    "current_time = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "train_log_dir = './logs/' + current_time + '/train'\n",
    "checkpoint_dir = './training_checkpoints/PDE_D12-K37-H7'#检查点读取、保存路径\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "\n",
    "\n",
    "def itas():\n",
    "    #读取冷却效率\n",
    "    itas = []\n",
    "    for i in range(0,SAMPLE_NUM):\n",
    "        ita = tf.image.crop_to_bounding_box(plt.imread(input_path+str(i+1)+'.png'), 198, 0, 126, 522)\n",
    "        ita = ita[:,:,0]\n",
    "        ita = ita[...,tf.newaxis]\n",
    "        ita = tf.image.resize(ita,[size1, size2])\n",
    "        itas.append(ita)\n",
    "    return itas\n",
    "\n",
    "def BR():\n",
    "    #读取吹风比\n",
    "    BR = []\n",
    "    for i in range(0,SAMPLE_NUM):\n",
    "        br = tf.image.crop_to_bounding_box(plt.imread(input_path+str(i+1)+'.png'), 198, 522, 126, 522)\n",
    "        br = br[:,:,1]\n",
    "        br = br[...,tf.newaxis]\n",
    "        br = tf.image.resize(br,[size1, size2])\n",
    "        BR.append(br)\n",
    "    return BR\n",
    "\n",
    "def G(): \n",
    "    #读取几何\n",
    "    G = []\n",
    "    for i in range(0,SAMPLE_NUM):\n",
    "        g = tf.image.crop_to_bounding_box(plt.imread(input_path+str(i+1)+'.png'), 198, 522, 126, 522)\n",
    "        g = g[:,:,1]\n",
    "        g = g.numpy()\n",
    "        g = np.int64(g>0)\n",
    "        g = tf.cast(g, tf.float32)\n",
    "        g = g[...,tf.newaxis]\n",
    "        g = tf.image.resize(g,[size1, size2])\n",
    "        G.append(g)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb754f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "itas = itas()\n",
    "BR = BR()  \n",
    "G = G()\n",
    "\n",
    "for i in range(SAMPLE_NUM):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(0.8 - itas[i], cmap=plt.cm.jet, vmin=0, vmax=0.8)\n",
    "    plt.axis('off') \n",
    "    plt.show()\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(G[i], cmap=plt.cm.gray,vmin=0, vmax=1)\n",
    "    plt.axis('off') \n",
    "    plt.show()\n",
    "\n",
    "inlet = np.zeros((size1, size2))\n",
    "exit = np.zeros((size1, size2))\n",
    "\n",
    "inlet[:, 0] = 1\n",
    "exit[:, -1] = 1\n",
    "\n",
    "inlet = inlet[...,tf.newaxis]\n",
    "exit = exit[...,tf.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5255993",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputSet:\n",
    "    def __init__(self, path0):\n",
    "        self.input_path = path0\n",
    "      \n",
    "    def loaddata(self):\n",
    "        in0 = np.zeros((SAMPLE_NUM, size1, size2, bc_channel))\n",
    "        out0 = np.zeros((SAMPLE_NUM, size1, size2, out_channel))\n",
    "        \n",
    "        for i in range(0,SAMPLE_NUM):\n",
    "            in0[i,:,:,0:1] = G[i]\n",
    "            in0[i,:,:,1:2] = BR[i]\n",
    "            in0[i,:,:,2:3] = inlet\n",
    "            in0[i,:,:,3:4] = exit\n",
    "            out0[i,:,:,0:1] = itas[i]\n",
    "        \n",
    "        self.in0 = in0\n",
    "        self.out0 = out0\n",
    "        \n",
    "        print(\"shape of all_input:\" + str(in0.shape))\n",
    "        print(\"shape of all_output:\" + str(out0.shape))\n",
    "        \n",
    "    def create_input_dataset(self):\n",
    "        # Split Training and Test Dataset\n",
    "        train = self.all_input[:train_num, ...]\n",
    "        test = self.all_input[train_num:, ...]   \n",
    "        # Change Ndarray to Tensor\n",
    "        train = tf.cast(train, tf.float32)\n",
    "        test = tf.cast(test, tf.float32)\n",
    "        return train, test\n",
    "    \n",
    "    def create_output_dataset(self):\n",
    "        # Split Training and Test Dataset\n",
    "        train = self.all_output[:train_num, ...]\n",
    "        test = self.all_output[train_num:, ...]\n",
    "        # Change Ndarray to Tensor\n",
    "        train = tf.cast(train, tf.float32)\n",
    "        test = tf.cast(test, tf.float32)\n",
    "        return train, test\n",
    "    \n",
    "    def generate_dataset(self):\n",
    "        input_train, input_test = self.create_input_dataset()\n",
    "        output_train, output_test = self.create_output_dataset()\n",
    "        input_train_dataset = tf.data.Dataset.from_tensor_slices(input_train)\n",
    "        input_test_dataset = tf.data.Dataset.from_tensor_slices(input_test)\n",
    "\n",
    "        output_train_dataset = tf.data.Dataset.from_tensor_slices(output_train)\n",
    "        output_test_dataset = tf.data.Dataset.from_tensor_slices(output_test)\n",
    "\n",
    "        train_dataset = tf.data.Dataset.zip((input_train_dataset, output_train_dataset))\n",
    "        test_dataset = tf.data.Dataset.zip((input_test_dataset, output_test_dataset))\n",
    "\n",
    "        train_dataset = train_dataset.batch(1).shuffle(train_num)\n",
    "        test_dataset = test_dataset.batch(1)\n",
    "        return train_dataset, test_dataset\n",
    "\n",
    "    def create(self):\n",
    "        self.loaddata()\n",
    "        self.all_input = self.in0\n",
    "        self.all_output = self.out0\n",
    "        self.train_dataset, self.test_dataset = self.generate_dataset()\n",
    "    \n",
    "    def flip_up_down(self):\n",
    "        #创建一个由原数据集上下翻转后得到的数据集\n",
    "        self.loaddata()\n",
    "        self.all_input = tf.image.flip_up_down(self.in0)\n",
    "        self.all_output = tf.image.flip_up_down(self.out0)\n",
    "        self.train_dataset, self.test_dataset = self.generate_dataset()\n",
    "        \n",
    "    def flip_left_right(self):\n",
    "        #创建一个由原数据集左右翻转后得到的数据集\n",
    "        self.loaddata()\n",
    "        self.all_input = tf.image.flip_left_right(self.in0)\n",
    "        self.all_output = tf.image.flip_left_right(self.out0)\n",
    "        self.train_dataset, self.test_dataset = self.generate_dataset()\n",
    "    \n",
    "    def rot90(self):\n",
    "        #创建一个由原数据集顺时针旋转90°后得到的数据集\n",
    "        self.loaddata()\n",
    "        self.all_input = tf.image.rot90(self.in0,3)\n",
    "        self.all_output = tf.image.rot90(self.out0,3)\n",
    "        self.train_dataset, self.test_dataset = self.generate_dataset()\n",
    "        \n",
    "    def crop(self):\n",
    "        #创建一个由原数据集剪切后得到的数据集\n",
    "        self.loaddata()\n",
    "        self.all_input = tf.image.crop_to_bounding_box(self.in0, 0, int(size1/4),size1,int(size2/2))\n",
    "        self.all_output = tf.image.crop_to_bounding_box(self.out0,0, int(size1/4),size1,int(size2/2))\n",
    "        self.train_dataset, self.test_dataset = self.generate_dataset()\n",
    "    \n",
    "    def resize(self):\n",
    "        #创建一个由原数据集缩放后得到的数据集\n",
    "        self.loaddata()\n",
    "        self.all_input = tf.image.resize(self.in0,[int(size1 * 2), int(size2 * 2)])\n",
    "        self.all_output = tf.image.resize(self.out0,[int(size1 * 2), int(size2 * 2)])\n",
    "        self.train_dataset, self.test_dataset = self.generate_dataset()\n",
    "    \n",
    "    def add0(self):\n",
    "        #创建一个在原数据集前补若干列0后得到的数据集\n",
    "        self.loaddata_1()\n",
    "        self.all_input = self.in1\n",
    "        self.all_output = self.out1\n",
    "        self.train_dataset, self.test_dataset = self.generate_dataset()\n",
    "    \n",
    "    def display(self):\n",
    "        for example_input1, example_target in self.train_dataset.take(1):\n",
    "            display_list = [example_input1[0, ... , 0], example_target[0, ... , 0]]  \n",
    "            for i in range(len(display_list)):\n",
    "                plt.subplot(1, len(display_list), i + 1)        \n",
    "                plt.imshow(display_list[i], cmap=plt.cm.gray,vmin=0, vmax=1)\n",
    "                plt.axis('off')\n",
    "                #plt.colorbar()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0feea30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "set3 = InputSet(input_path)\n",
    "set3.create()\n",
    "print(set3.train_dataset)\n",
    "set3.display()\n",
    "\n",
    "# set4 = InputSet(input_path)\n",
    "# set4.flip_up_down()\n",
    "# print(set4.train_dataset)\n",
    "# set4.display()\n",
    "\n",
    "# set5 = InputSet(input_path)\n",
    "# set5.flip_left_right()\n",
    "# print(set5.train_dataset)\n",
    "# set5.display()\n",
    "\n",
    "# set6 = InputSet(input_path)\n",
    "# set6.rot90()\n",
    "# print(set6.train_dataset)\n",
    "# set6.display()\n",
    "\n",
    "# set7 = InputSet(input_path)\n",
    "# set7.crop()\n",
    "# print(set7.train_dataset)\n",
    "# set7.display()\n",
    "\n",
    "# set8 = InputSet(input_path)\n",
    "# set8.resize()\n",
    "# print(set8.train_dataset)\n",
    "# set8.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c34ac5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generator():\n",
    "    input1 = tf.keras.layers.Input(shape=INPUT1_SHAPE)  \n",
    "    output_pre = tf.keras.layers.Input(shape=OUTPUT_SHAPE) \n",
    "    input_hs = tf.keras.layers.Input(shape=HS_SHAPE) \n",
    "    \n",
    "    inputc = tf.keras.layers.concatenate([input1,output_pre,input_hs])      \n",
    "    \n",
    "    # 卷积核初始化，均值和标准差\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "    y = tf.keras.layers.Conv2D(DE_NUM,\n",
    "                               DE_kernel_size,\n",
    "                               strides=1,\n",
    "                               padding='same',\n",
    "                               kernel_initializer=initializer,\n",
    "                               use_bias=False)(inputc)\n",
    "    y = tf.keras.layers.BatchNormalization()(y)\n",
    "    y = tf.keras.layers.LeakyReLU()(y)\n",
    "    \n",
    "    y = tf.keras.layers.Conv2D(out_channel+hs_channel,\n",
    "                               1,\n",
    "                               strides=1,\n",
    "                               padding='same',\n",
    "                               kernel_initializer=initializer,\n",
    "                               use_bias=False)(y)\n",
    "    #conv_DE.add(tf.keras.layers.BatchNormalization())\n",
    "    y = tf.keras.layers.LeakyReLU()(y)       \n",
    "    y = output_pre + y\n",
    "\n",
    "    return tf.keras.Model(inputs=[input1, output_pre, input_hs], outputs=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac5af1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "Gen_loss = tf.keras.metrics.Mean('Gen_loss')\n",
    "# SSIM_loss = tf.keras.metrics.Mean('SSIM_loss')\n",
    "L1_loss = tf.keras.metrics.Mean('L1_loss')\n",
    "\n",
    "generator_optimizer = tf.keras.optimizers.Adam(G_step_size, beta_1=0.5)\n",
    "\n",
    "generator = Generator()\n",
    "generator.summary()\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,generator=generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148425c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(gen_output, target):       \n",
    "    l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n",
    "    # ssim_loss = 1 - tf.image.ssim(target, gen_output, 1)\n",
    "    total_gen_loss =  (LAMBDA * l1_loss)\n",
    "    return total_gen_loss, l1_loss\n",
    "\n",
    "# 使用AutoGraph转换成静态图表示，加速代码\n",
    "tf.config.experimental_run_functions_eagerly(True)\n",
    "\n",
    "@tf.function\n",
    "def train_step(input1, ini_output, target):          \n",
    "    with tf.GradientTape() as gen_tape:        \n",
    "        gen_output, gen_loss, l1_loss = iteration_net(input1, ini_output, target)     #ini_output，  \n",
    "\n",
    "    generator_gradients = gen_tape.gradient(gen_loss, generator.trainable_variables)   \n",
    "    generator_optimizer.apply_gradients(zip(generator_gradients, generator.trainable_variables))\n",
    "    \n",
    "    Gen_loss(gen_loss)\n",
    "    L1_loss(l1_loss)\n",
    "\n",
    "def iteration_net(input1, ini_output, target):\n",
    "    gen_loss = 0\n",
    "    l1_loss = 0\n",
    "    \n",
    "    ini_hs = tf.tile(tf.zeros_like(input1[:,:,:,0:1]),[1,1,1,hs_channel])\n",
    "    display_list = []\n",
    "    gen_output = generator([input1, ini_output, ini_hs], training=True)\n",
    "        \n",
    "    for i in range(Iteration_NUM):        \n",
    "        hs = gen_output[:,:,:,0:hs_channel]\n",
    "        output = gen_output[:,:,:,hs_channel:hs_channel+out_channel]\n",
    "        gen_output = generator([input1, output, hs], training=True)\n",
    "        gen_loss0,l1_loss0 = generator_loss(output, target)\n",
    "        gen_loss = gen_loss + gen_loss0\n",
    "        l1_loss = l1_loss + l1_loss0\n",
    " \n",
    "    return output, gen_loss, l1_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f0cf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boundary_condition(input_1, target):    \n",
    "    input_1 = input_1.numpy()     \n",
    "    input_1 = tf.cast(input_1, tf.float32)\n",
    "\n",
    "    target = target   \n",
    "\n",
    "    ini_output = tf.tile(tf.zeros_like(input_1[:,:,:,0:1]),[1,1,1,out_channel])\n",
    "    ini_output = tf.add(ini_output * 0, 0.0*tf.random.normal(shape = ini_output.shape))         \n",
    "    ini_output = tf.cast(ini_output, tf.float32)\n",
    "    \n",
    "    return input_1, ini_output, target\n",
    "\n",
    "def generate_images(test_input1, test_ini_output, test_tar, name, figsize=(size1, size2)):\n",
    "\n",
    "    #test_ini_hs = tf.tile(tf.zeros_like(test_input1[:,:,:,0:1]),[1,1,1,hs_channel])\n",
    "    prediction, gen_loss, l1_loss = iteration_net(test_input1, test_ini_output, test_tar)\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    display_list = [test_input1[0, ... , 0],\n",
    "                    0.8 - test_tar[0, ... , 0] - (test_input1[0, ... , 0]), \n",
    "                    0.8 - prediction[0, ... , 0] - (test_input1[0, ... , 0]), \n",
    "                    abs(test_tar[0, ... , 0] - prediction[0, ... , 0]) * (1-test_input1[0, ... , 0])]\n",
    "    title = ['Holes', 'Target', 'Predicted', 'AbsError']\n",
    "    \n",
    "    l1_loss = tf.reduce_mean(tf.abs(test_tar[0, ... , 0] - prediction[0, ... , 0]) * (1-test_input1[0, ... , 0]))\n",
    "    \n",
    "    for i in range(len(display_list)):\n",
    "        if i == 0:\n",
    "            plt.subplot(1, 5, i + 1)\n",
    "            plt.title(title[i])\n",
    "            plt.imshow(display_list[i], cmap=plt.cm.gray,vmin=0, vmax=1)\n",
    "            plt.axis('off')\n",
    "        if i == 3:\n",
    "            plt.subplot(1, 5, i + 1)\n",
    "            plt.title(title[i])\n",
    "            plt.imshow(display_list[i], cmap=plt.cm.jet, vmin=0.0, vmax=1)\n",
    "            plt.axis('off') \n",
    "        elif (i == 1) or (i == 2):\n",
    "            plt.subplot(1, 5, i + 1)\n",
    "            plt.title(title[i])\n",
    "            plt.imshow(display_list[i], cmap=plt.cm.jet, vmin=0, vmax=0.8)\n",
    "            plt.axis('off')\n",
    "            \n",
    "\n",
    "    #保存图片（训练时注释掉）\n",
    "#     name = str(name) + '_result.jpg'\n",
    "#     path = os.path.join(save_path, name)\n",
    "#     plt.savefig(path)\n",
    "    plt.show()\n",
    "    return l1_loss\n",
    "    \n",
    "def training(epochs, ispadding=False):\n",
    "    \n",
    "    print('Training !\\n')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Train\n",
    "        \n",
    "        for input_1, target in set3.train_dataset:\n",
    "            input_1, ini_output, target = boundary_condition(input_1, target)\n",
    "            train_step(input_1, ini_output, target)\n",
    "            \n",
    "#         for input_1, target in set4.train_dataset:\n",
    "#             input_1, ini_output, target = boundary_condition(input_1, target)\n",
    "#             train_step(input_1, ini_output, target)\n",
    "\n",
    "#         for input_1, target in set5.train_dataset:\n",
    "#             input_1, ini_output, target = boundary_condition(input_1, target)\n",
    "#             train_step(input_1, ini_output, target)\n",
    "\n",
    "#         for input_1, target in set6.train_dataset:\n",
    "#             input_1, ini_output, target = boundary_condition(input_1, target)\n",
    "#             train_step(input_1, ini_output, target)\n",
    "\n",
    "#         for input_1, target in set7.train_dataset:\n",
    "#             input_1, ini_output, target = boundary_condition(input_1, target)\n",
    "#             train_step(input_1, ini_output, target)\n",
    "\n",
    "#         for input_1, target in set8.train_dataset:\n",
    "#             input_1, ini_output, target = boundary_condition(input_1, target)\n",
    "#             train_step(input_1, ini_output, target)\n",
    "#         print('Epoch {} completed'.format(epoch+1))\n",
    "\n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print('Epoch {}: Gen_loss is {:.3f}, L1_loss is {:.5f}'\n",
    "                  .format(epoch + 1, Gen_loss.result(), L1_loss.result()))\n",
    "            \n",
    "        if (epoch + 1) % 200 == 0:\n",
    "            for example_input1, example_target in set3.train_dataset.take(2):\n",
    "                input_1, ini_output, target = boundary_condition(example_input1, example_target)\n",
    "                generate_images(input_1, ini_output, target, '0')\n",
    "            for example_input1, example_target in set3.test_dataset.take(2):\n",
    "                input_1, ini_output, target = boundary_condition(example_input1, example_target)\n",
    "                generate_images(input_1, ini_output, target, '0') \n",
    "        \n",
    "        if (epoch + 1) > 40 and (epoch + 1) % 50 == 0:\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "        \n",
    "        Gen_loss.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e81ab0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载预训练模型\n",
    "# checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bea803",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training(3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eabe674",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "L1 = []\n",
    "for example_input1, example_target in set3.train_dataset.take(train_num):\n",
    "        input_1, ini_output, target = boundary_condition(example_input1, example_target)\n",
    "        l1_loss = generate_images(input_1, ini_output, target, str(i+1))\n",
    "        L1.append(float(l1_loss))\n",
    "        i = i + 1\n",
    "print(sum(L1)/len(L1))\n",
    "# a = np.array(L1)\n",
    "# np.savetxt(\"PDE_D12-K37-H7.txt\",a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849373c4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "L1 = []\n",
    "for example_input1, example_target in set3.test_dataset.take(test_num):\n",
    "    input_1, ini_output, target = boundary_condition(example_input1, example_target)\n",
    "    l1_loss = generate_images(input_1, ini_output, target, str(i+1)) \n",
    "    L1.append(float(l1_loss))\n",
    "    i = i + 1\n",
    "print(sum(L1)/len(L1))\n",
    "# a = np.array(L1)\n",
    "# np.savetxt(\"PDE_D12-K37-H7-test.txt\",a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa85c53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
